{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Summarizer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8IXrX1XeHbDQ"},"source":["# In class task 2 (ICE-2)"]},{"cell_type":"markdown","metadata":{"id":"YSjDHSGUHdLe"},"source":["### **Team Members :**\n","*   Aditya Pujari (11491374)\n","*   Gagan Sai Ram Anvesh Achanta (11447940)\n","*   Praveen Kumar somara (11525451)\n","*   Sai Pranathi Karedla (11533059)\n","*   Brinda Potluri (11526591)"]},{"cell_type":"markdown","metadata":{"id":"7u98Emg6CWf4"},"source":["# Mount Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DngpZD-sA6Z-","executionInfo":{"status":"ok","timestamp":1631552409625,"user_tz":300,"elapsed":144,"user":{"displayName":"Aditya Pujari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1I3G9BMqT1pluoDvZFTn8_BCzn-r6SSs7jgt4qA=s64","userId":"05634372435003585479"}},"outputId":"37f60c90-7563-4f73-8378-8ecae625f027"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"RRjoxuAzLSdH"},"source":["import os\n","ROOT = r'/content/drive/MyDrive/College/CSCE 5222 - Feature Engineering/ICA 2'\n","os.chdir(ROOT)\n","assert os.getcwd() == ROOT"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ImcGJyFnMN_d"},"source":["# Installing necessary libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNOGV8n5MQ1j","executionInfo":{"status":"ok","timestamp":1631552410584,"user_tz":300,"elapsed":962,"user":{"displayName":"Aditya Pujari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1I3G9BMqT1pluoDvZFTn8_BCzn-r6SSs7jgt4qA=s64","userId":"05634372435003585479"}},"outputId":"f215daf4-cf83-47e7-eeea-e89be626f028"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"v0fUa9YcEfuc"},"source":["# Import all necessary libraries"]},{"cell_type":"code","metadata":{"id":"64nCW7jFEGHI"},"source":["from nltk.corpus import stopwords\n","from nltk.cluster.util import cosine_distance\n","import numpy as np\n","import re\n","import networkx as nx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gk_3Zwp9Kr0O"},"source":["# File paths"]},{"cell_type":"code","metadata":{"id":"EgkvU3CVKuWm"},"source":["articles_directory_path = \"dataset\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L48uwRtPEx4c"},"source":["# Generate clean sentences"]},{"cell_type":"code","metadata":{"id":"0Jx1aFHME3GQ"},"source":["def read_article(file_name):\n","    file = open(file_name, \"r\")\n","    filedata = file.readlines()\n","    article = filedata[0].split(\".\")\n","    # print(article)\n","    sentences = []\n","    for sentence in article:\n","        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","    #print(sentences)\n","    sentences.pop() \n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcNXYl0lFK0c"},"source":["# Similarity Matrix"]},{"cell_type":"code","metadata":{"id":"2S6DTZHCFNtA"},"source":["def build_similarity_matrix(sentences, stop_words):\n","    # Create an empty similarity matrix\n","    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n"," \n","    for idx1 in range(len(sentences)):\n","        for idx2 in range(len(sentences)):\n","            if idx1 == idx2: #ignore if both are same sentences\n","                continue \n","            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n","\n","    return similarity_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VD4t9k3qFVkk"},"source":["# Sentence Similarity"]},{"cell_type":"code","metadata":{"id":"a-tGnx_AE96O"},"source":["def sentence_similarity(sent1, sent2, stopwords=None):\n","    if stopwords is None:\n","        stopwords = []\n"," \n","    sent1 = [w.lower() for w in sent1]\n","    sent2 = [w.lower() for w in sent2]\n"," \n","    all_words = list(set(sent1 + sent2))\n"," \n","    vector1 = [0] * len(all_words)\n","    vector2 = [0] * len(all_words)\n"," \n","    # build the vector for the first sentence\n","    for w in sent1:\n","        if w in stopwords:\n","            continue\n","        vector1[all_words.index(w)] += 1\n"," \n","    # build the vector for the second sentence\n","    for w in sent2:\n","        if w in stopwords:\n","            continue\n","        vector2[all_words.index(w)] += 1\n"," \n","    return 1 - cosine_distance(vector1, vector2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvmD_jpNE7RU"},"source":["# Output 1"]},{"cell_type":"markdown","metadata":{"id":"UzP3fxQlCe1Z"},"source":["## Generate Summary"]},{"cell_type":"code","metadata":{"id":"VlMVNeXPFWPc"},"source":["def generate_summary(file_name, top_n=5):\n","    stop_words = stopwords.words('english')\n","    summarize_text = []\n","\n","    # Step 1 - Read text anc split it\n","    sentences =  read_article(file_name)\n","\n","    # Step 2 - Generate Similary Martix across sentences\n","    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n","\n","    # Step 3 - Rank sentences in similarity martix\n","    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n","    scores = nx.pagerank(sentence_similarity_graph)\n","\n","    # Step 4 - Sort the rank and pick top sentences\n","    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)   \n","    #print(\"Indexes of top ranked_sentence order are \"+str(file_name), ranked_sentence)    \n","\n","    for i in range(top_n):\n","      # print(i)\n","      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n","\n","    # Step 5 - Offcourse, output the summarize texr\n","    #print(\"\\ntext :+++++++++++++++++++++++++++++\",summarize_text)\n","    #print(\"Summarize Text: \\n\"+str(file_name), \". \".join(summarize_text))\n","    return (\". \".join(summarize_text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fuomIMRLFEER"},"source":["## Output_1 : Final Output"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1s9ZYcPEHaS8","executionInfo":{"status":"ok","timestamp":1631552410956,"user_tz":300,"elapsed":119,"user":{"displayName":"Aditya Pujari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1I3G9BMqT1pluoDvZFTn8_BCzn-r6SSs7jgt4qA=s64","userId":"05634372435003585479"}},"outputId":"e890a8a0-ad64-453e-84e3-3cedbc6893b4"},"source":["for article_no in range(1,6):\n","    summarized_text = generate_summary(articles_directory_path+\"/article_\"+str(article_no)+\".txt\",2)\n","    with open('output1.txt','a') as outfile:\n","        outfile.write(\"Summary of article_\"+str(article_no)+':'+'\\n\\n')\n","        print(\"Summary of article_\"+str(article_no)+':'+'\\n\\n')\n","        outfile.write(summarized_text.strip()+'\\n\\n')\n","        print(summarized_text.strip()+'\\n\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary of article_1:\n","\n","\n","Posts from accounts in the network spread memes that used images from the Planet of Apes films to give the impression the vaccine would turn people into monkeys. In its latest report on \"coordinated inauthentic behaviour\", Facebook said it found links between the network and a botched disinformation campaign from influencer marketing agency Fazze - which is part of a Russian-based company called AdNow\n","\n","\n","Summary of article_2:\n","\n","\n","The video has had close to 400,000 views on Twitter.  They claim fake syringes with \"disappearing needles\" are being used in an attempt by the authorities to promote a vaccine that doesn't exist\n","\n","\n","Summary of article_3:\n","\n","\n","The accounts uncovered by the BBC also share similarities with a resurgent pro-Chinese network spotted by the social media analytics firm Graphika earlier this year. Hundreds of fake or hijacked social media accounts have been pushing pro-Chinese government messages about the coronavirus pandemic on Facebook, Twitter and YouTube, a BBC investigation has found\n","\n","\n","Summary of article_4:\n","\n","\n","This has prompted some doctors and scientists to push back against online misinformation on the very platforms where it thrives.  They take statistics not validated by scientists and put them online\n","\n","\n","Summary of article_5:\n","\n","\n","Their joint channel has 840,000 subscribers - and MxR has a personal following of more than two million.  A channel receives a strike against it if a copyright owner formally notifies YouTube that a copyright infringement has taken place\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9XNLonOrAgCx"},"source":["# Output2"]},{"cell_type":"markdown","metadata":{"id":"6zQ41Z7oCtem"},"source":["## Text Cleaning"]},{"cell_type":"code","metadata":{"id":"8udQX3WqAgCx"},"source":["def Text_cleaning(sentence):\n","    \n","    # replacing all the \"-\" special charater with ''\n","    sentence=sentence.replace('-','')\n","    \n","    # Replacing double spaces with single Space\n","    sentence = sentence.replace('  ',' ')\n","    \n","    # Removing Punctuation marks\n","    punctuation_signs = list(\"?:!,;\")\n","    for punct_sign in punctuation_signs:\n","        sentence = sentence.replace(punct_sign, '')\n","        \n","    # Removing possessive pronoun's\n","    sentence = sentence.replace(\"'s\",\"\")\n","    \n","    #Removing \"\n","    sentence = sentence.replace('\"',\"\")\n","    \n","    #converting all the words into same case i.e., Upper Case or Lower Case because Ball and ball indicates the same meaning\n","    \n","    sentence = sentence.lower()\n","    \n","    # Removing StopWords\n","    stop_words = list(stopwords.words('english'))\n","    #print(stop_words)\n","    for stop_word in stop_words:\n","        stopword = r\"\\b\" + stop_word + r\"\\b\"\n","        sentence = sentence.replace(stopword,\"\")\n","        \n","    #Removing special characters like \"\\n\",\"\\r\"\n","    sentence = sentence.replace('\\n','')\n","    sentence = sentence.replace('\\r','')\n","    \n","    #print(sentence)\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GHGyP-_4AgC0"},"source":["def read_article1(file_name):\n","    file = open(file_name, \"r\")\n","    filedata = file.readlines()\n","    article = filedata[0]\n","    article = Text_cleaning(article)\n","    #regex = r\"-\"\n","    #article = re.sub(regex,\"\",article)\n","    #article.str.replace(r\" - \",\"\")\n","    article = article.split(\".\")\n","    sentences = []\n","    for sentence in article:\n","        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n","        #sentences.append(sentence.split(\" \"))\n","    #print(sentences)\n","    sentences.pop() \n","    return sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w1zAlXiKAgC3"},"source":["## Generate Summary"]},{"cell_type":"code","metadata":{"id":"mywcZAQfAgC4"},"source":["def generate_summary1(file_name, top_n=5):\n","    stop_words = stopwords.words('english')\n","    summarize_text = []\n","\n","    # Step 1 - Read text anc split it\n","    sentences =  read_article1(file_name)\n","    # print(\"Sentences : \"+''.join(sentences))\n","    \n","    \n","    # Step 2 - Generate Similary Martix across sentences\n","    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n","\n","    # Step 3 - Rank sentences in similarity martix\n","    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n","    scores = nx.pagerank(sentence_similarity_graph)\n","\n","    # Step 4 - Sort the rank and pick top sentences\n","    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)   \n","    #print(\"Indexes of top ranked_sentence order are \"+str(file_name), ranked_sentence)    \n","\n","    for i in range(top_n):\n","      # print(i)\n","      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n","\n","    # Step 5 - Offcourse, output the summarize texr\n","    #print(\"\\ntext :+++++++++++++++++++++++++++++\",summarize_text)\n","    #print(\"Summarize Text: \\n\"+str(file_name), \". \".join(summarize_text))\n","    return (\". \".join(summarize_text))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKH2NRmNAgC5"},"source":["## Output_2 : Final Output"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"71B0Wp2pAgC6","executionInfo":{"status":"ok","timestamp":1631552411414,"user_tz":300,"elapsed":147,"user":{"displayName":"Aditya Pujari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg1I3G9BMqT1pluoDvZFTn8_BCzn-r6SSs7jgt4qA=s64","userId":"05634372435003585479"}},"outputId":"7e69f310-f2ac-48dd-ff3a-052143a88cda"},"source":["for article_no in range(1,6):\n","    summarized_text = generate_summary1(articles_directory_path+\"/article_\"+str(article_no)+\".txt\",2)\n","    with open('output2.txt','a') as outfile:\n","        outfile.write(\"Summary of article_\"+str(article_no)+':'+'\\n\\n')\n","        print(\"Summary of article_\"+str(article_no)+':'+'\\n\\n')\n","        outfile.write(summarized_text.strip()+'\\n\\n')\n","        print(summarized_text.strip()+'\\n\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary of article_1:\n","\n","\n","facebook said it had removed 65 facebook accounts and 243 instagram accounts for violating their policy against foreign interference. posts from accounts in the network spread memes that used images from the planet of apes films to give the impression the vaccine would turn people into monkeys\n","\n","\n","Summary of article_2:\n","\n","\n","they claim fake syringes with disappearing needles are being used in an attempt by the authorities to promote a vaccine that doesn't exist.  the video has had close to 400000 views on twitter\n","\n","\n","Summary of article_3:\n","\n","\n","the accounts uncovered by the bbc also share similarities with a resurgent prochinese network spotted by the social media analytics firm graphika earlier this year. hundreds of fake or hijacked social media accounts have been pushing prochinese government messages about the coronavirus pandemic on facebook twitter and youtube a bbc investigation has found\n","\n","\n","Summary of article_4:\n","\n","\n","with viral misinformation about the deadly coronavirus in china spreading rapidly online some doctors and scientists have taken to social media to fight back against false reports. this has prompted some doctors and scientists to push back against online misinformation on the very platforms where it thrives\n","\n","\n","Summary of article_5:\n","\n","\n","mxr and potastic panda the youtubers who host the channel say they have received four copyright claims in the same bill from jukin media with google yet to be notified.  their joint channel has 840000 subscribers and mxr has a personal following of more than two million\n","\n","\n"]}]}]}